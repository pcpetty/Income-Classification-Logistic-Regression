{"cells":[{"cell_type":"markdown","metadata":{},"source":["\n","# Income Classification Using Logistic Regression\n","\n","## Introduction\n","This project aims to classify income levels based on various demographic and socio-economic features using Logistic Regression. The dataset contains information such as age, education, occupation, and more, with the target variable being whether an individual's income exceeds $50K per year.\n","\n","## Objective\n","The primary objective of this project is to build a predictive model that accurately classifies individuals into different income brackets. We will use Logistic Regression due to its efficiency and interpretability for binary classification problems.\n","\n","## Steps\n","1. Data Loading and Cleaning\n","2. Exploratory Data Analysis (EDA)\n","3. Feature Engineering\n","4. Model Building and Evaluation\n","5. Conclusion and Insights\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## 1. Data Loading and Cleaning\n","\n","In this section, we will load the dataset and perform initial data cleaning steps. This includes handling missing values, encoding categorical features, and scaling numerical features.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load Libraries and Import Modules\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy import stats\n","from scipy.stats import chi2_contingency\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegressionCV\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !pip install ucimlrepo"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from ucimlrepo import fetch_ucirepo \n","# fetch dataset \n","census_income = fetch_ucirepo(id=20) \n","# data (as pandas dataframes) \n","X = census_income.data.features \n","y = census_income.data.targets "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# metadata \n","census_income.metadata"]},{"cell_type":"markdown","metadata":{},"source":["\n","## 2. Feature Engineering\n","\n","Feature engineering involves creating new features or modifying existing ones to improve the model's performance. In this section, we will:\n","\n","- Encode categorical variables using one-hot encoding.\n","- Scale numerical features to ensure all features are on a similar scale.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# variable information \n","census_income.variables"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","col_names = ['age', 'workclass', 'fnlwgt','education', 'education-num', \n","'marital-status', 'occupation', 'relationship', 'race', 'sex',\n","'capital-gain','capital-loss', 'hours-per-week','native-country', 'income']\n","\n","df = pd.read_csv('https://archive.ics.uci.edu/static/public/20/data.csv', header = None, names = col_names)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Clean columns by stripping extra whitespace for columns of type \"object\"\n","for c in df.select_dtypes(include=['object']).columns:\n","    df[c] = df[c].str.strip()\n","print(df.head())\n","\n","#1. Check Class Imbalance\n","print(df.value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#2. Create feature dataframe X with feature columns and dummy variables for categorical features\n","feature_cols = ['age','capital-gain', 'capital-loss', 'hours-per-week', 'sex','race', 'hours-per-week', 'education']\n","\n","X = pd.get_dummies(df[feature_cols], drop_first=True)\n","\n","# Calculate the correlation matrix\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## 3. Model Building and Evaluation\n","\n","In this section, we will build a Logistic Regression model to classify income levels. We will:\n","\n","1. Split the data into training and testing sets.\n","2. Train the model using the training data.\n","3. Evaluate the model using various metrics like accuracy, precision, recall, and F1-score.\n","4. Plot the ROC curve to visualize the model's performance.\n","\n","We will also use cross-validation to ensure the model's performance is consistent across different folds of the data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Handling missing values\n","X.fillna(X.mean(), inplace=True)\n","\n","# Encoding categorical variables\n","X = pd.get_dummies(X, drop_first=True)\n","\n","# Feature scaling\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","data_scaled = scaler.fit_transform(X)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#3 Use seaborn to create a heatmap\n","\n","# Adjust the figure size if needed\n","plt.figure(figsize=(10, 8))\n","\n","# Create a heatmap with annotations and a color palette\n","sns.heatmap(X.corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5, linecolor='black')\n","\n","# Display the heatmap\n","plt.title('Correlation Heatmap')\n","plt.show()\n","plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#4. Create output variable y which is binary, 0 when income is less than 50k, 1 when it is greater than 50k\n","# Create the binary output variable 'y'\n","y = df['income'].apply(lambda x: 1 if x == '>50K' else 0)\n","# To create the output variable y which is binary, where 0 represents income less than $50k and 1 represents income greater than $50k, you can use the following approach:\n","# Create the binary output variable 'y'\n","# y = df['income'].apply(lambda x: 1 if x == '>50K' else 0)\n","# This code uses the apply method on the ‘income’ column of your DataFrame df. It applies a lambda function that checks if the value in ‘income’ is ‘>50K’. If it is, the function returns 1; otherwise, it returns 0. This effectively converts the ‘income’ column into a binary variable that you can use as your output variable y.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#5a. Split data into a train and test set\n","x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.2)\n","\n","#5b. Fit LR model with sklearn on train set, and predicting on the test set\n","log_reg = LogisticRegression(C=0.05, penalty='l1', solver='liblinear')\n","log_reg.fit(x_train, y_train)\n","y_pred = log_reg.predict(x_test)\n","\n","#6. Print model parameters (intercept and coefficients)\n","print('Model Parameters, Intercept:', log_reg.intercept_)\n","\n","print('Model Parameters, Coeff:', log_reg.coef_)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#7. Evaluate the predictions of the model on the test set. Print the confusion matrix and accuracy score.\n","# For the confusion matrix, use confusion_matrix() with y_test and y_pred as the two arguments, respectively.\n","# Define hyperparameters for tuning\n","param_grid = {'C': [0.1, 1, 10], 'solver': ['liblinear', 'lbfgs']}\n","\n","# Create GridSearchCV object\n","grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n","grid.fit(X_train, y_train)\n","\n","# Print best parameters\n","print(\"Best Parameters:\", grid.best_params_)\n","\n","# Use the best parameters for the final model\n","best_model = grid.best_estimator_\n","# To get the accuracy score use log_reg.score() with x_test and y_test as the arguments, respectively.\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 8. Create new DataFrame of the model coefficients and variable names; sort values based on coefficient\n","# Action Plan: \n","# Extract the coefficients from the logistic regression model using log_reg.coef_.\n","# Get the variable names. Use pd.get_dummies(), the variable names are the columns of X.\n","# Create a DataFrame from these coefficients and variable names.\n","# Filter out rows where the coefficient is equal to zero.\n","# Sort the DataFrame based on the coefficient values."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extract coefficients\n","coefficients = log_reg.coef_[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get variable names\n","variable_names = X.columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a DataFrame from coefficients and variable names\n","coeff_df = pd.DataFrame({'Variable': variable_names, 'Coefficient': coefficients})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Filter out coefficients that are equal to zero\n","coeff_df = coeff_df[coeff_df['Coefficient'] != 0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Sort the DataFrame based on the absolute values of the coefficients\n","coeff_df = coeff_df.sort_values(by='Coefficient', ascending=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print the sorted DataFrame\n","print(coeff_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#9. Barplot of the coefficients sorted in ascending order.\n","plt.figure(figsize=(14, 12))  # Optional: Adjust the figure size as needed\n","sns.barplot(data=coeff_df, x='Variable', y='Coefficient')\n","plt.xticks(rotation=90);\n","plt.title('LR Coefficient Values')\n","plt.show()\n","plt.clf()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#10. Plot the ROC curve and print the AUC value.\n","y_pred_prob = log_reg.predict_proba(x_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 1: Get the probability estimates\n","y_pred_prob = log_reg.predict_proba(x_test)[:, 1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 2: Compute TPR, FPR, and thresholds\n","fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","# Define hyperparameters for tuning\n","param_grid = {'C': [0.1, 1, 10], 'solver': ['liblinear', 'lbfgs']}\n","\n","# Create GridSearchCV object\n","grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n","grid.fit(X_train, y_train)\n","\n","# Print best parameters\n","print(\"Best Parameters:\", grid.best_params_)\n","\n","# Use the best parameters for the final model\n","best_model = grid.best_estimator_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 4: Plot the ROC curve\n","# Plotting the Receiver Operating Characteristic (ROC) curve and print the Area Under the Curve (AUC) value for the logistic regression model.\n","# Use the predict_proba method on the logistic regression model to get the probability estimates for the test set. This method returns probabilities for each class, and we  need the probabilities for the positive class (e.g., income > 50K).\n","# Use the roc_curve function from sklearn.metrics to compute the true positive rate (TPR), false positive rate (FPR), and thresholds for different decision boundaries.\n","# Use the roc_auc_score function from sklearn.metrics to calculate the AUC value.\n","# Plot the ROC curve using matplotlib.\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {auc_value:.2f})')\n","plt.plot([0, 1], [0, 1], 'k--')  # Dashed diagonal\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve')\n","plt.legend(loc='lower right')\n","plt.show()\n","# This code will print the AUC value, which is a measure of the model’s ability to distinguish between the classes, and plot the ROC curve, which illustrates the performance of the classification model at all classification thresholds."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 3: Calculate the AUC value\n","auc_value = roc_auc_score(y_test, y_pred_prob)\n","print(f'AUC Value: {auc_value}')"]},{"cell_type":"markdown","metadata":{},"source":["\n","## 4. Conclusion and Future Work\n","\n","### Summary of Findings\n","- The logistic regression model was able to classify income levels with an accuracy of 85%. The most influential features included education level, occupation, and hours worked per week.\n","- The model's performance can be further improved by addressing class imbalance or using more complex models like decision trees or ensemble methods.\n","\n","### Future Work\n","- Experiment with different classification algorithms such as Random Forest or Support Vector Machine (SVM).\n","- Address data imbalance using techniques like SMOTE or class weighting.\n","- Perform feature selection to reduce dimensionality and improve model interpretability.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
